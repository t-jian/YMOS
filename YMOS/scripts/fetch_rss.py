#!/usr/bin/env python3
"""
YMOS RSS æ•°æ®è·å–å·¥å…·ï¼ˆæŠ•èµ„åœºæ™¯ç¤ºä¾‹ï¼‰

ã€æœ¬è„šæœ¬æ˜¯æŠ•èµ„ç ”ç©¶åœºæ™¯çš„å®ç°ç¤ºä¾‹ã€‘
- æ•°æ®æºï¼šYongmai æŠ•èµ„æ´å¯Ÿåšå®¢ RSS + å…¶ä»–è´¢ç» RSS
- æ•°æ®ç±»å‹ï¼šå…è´¹å…¨æ–‡å¸‚åœºæ´å¯Ÿã€è¡Œä¸šåŠ¨æ€ã€å®è§‚ç»æµ

ã€å…¶ä»–åœºæ™¯å¦‚ä½•é€‚é…ã€‘
æœ¬è„šæœ¬æ˜¯ä¸‰å±‚æ¶æ„ã€Œç¬¬ä¸€å±‚ï¼šä¿¡æ¯è¾“å…¥ã€çš„å…·ä½“å®ç°ã€‚
å¦‚æœä½ æ˜¯å…¶ä»–é¢†åŸŸçš„çŸ¥è¯†å·¥ä½œè€…ï¼Œå¯ä»¥å‚è€ƒæœ¬è„šæœ¬çš„ç»“æ„ï¼Œæ›¿æ¢ä¸ºä½ çš„ RSS æºï¼š
- å­¦æœ¯ç ”ç©¶ â†’ arXiv RSS / Nature/Science RSS
- äº§å“ç»ç† â†’ Product Hunt RSS / TechCrunch RSS
- å†…å®¹åˆ›ä½œ â†’ å¾®åšçƒ­æœ RSS / çŸ¥ä¹çƒ­æ¦œ RSS

æ ¸å¿ƒä¸å˜ï¼šé€šè¿‡ RSS è‡ªåŠ¨æŠ“å–å†…å®¹åˆ°æœ¬åœ°ï¼Œæ— éœ€ API Key
æ ¸å¿ƒå¯å˜ï¼šRSS_SOURCES å­—å…¸ä¸­çš„è®¢é˜…æºåˆ—è¡¨

ã€æŠ•èµ„åœºæ™¯ã€‘å…è´¹æ•°æ®æºï¼š
https://yongmai.xyz/category/daily-research/feed/
Yongmai å¸‚åœºæ´å¯Ÿæ—¥æŠ¥ï¼Œæ¯æ—¥æ›´æ–°ï¼Œå…¨æ–‡è¾“å‡ºã€‚

å¦‚éœ€æ›´å¤šæŠ•èµ„ç±» RSS æºï¼Œå¯å‚è€ƒ Yongmai åšå®¢çš„æ¨èæ¸…å•ã€‚
"""

import urllib.request
import urllib.error
import xml.etree.ElementTree as ET
import argparse
import json
import ssl
import sys
from datetime import datetime, timedelta, timezone

# ã€æŠ•èµ„åœºæ™¯ã€‘é»˜è®¤ä½¿ç”¨ Yongmai å¸‚åœºæ´å¯Ÿ RSS
# ã€å…¶ä»–åœºæ™¯ã€‘ä¿®æ”¹ä¸ºä½ çš„ä¸»è¦æ•°æ®æº URL
DEFAULT_RSS_URL = "https://yongmai.xyz/category/daily-research/feed/"

# ============================================================
# ã€æŠ•èµ„åœºæ™¯ã€‘é»˜è®¤ RSS æ•°æ®æº
# ã€å…¶ä»–åœºæ™¯ã€‘ä¿®æ”¹ä¸ºä½ çš„é¢†åŸŸ RSS è®¢é˜…æº
# ============================================================
RSS_SOURCES = {
    "å¸‚åœºæ´å¯Ÿ": DEFAULT_RSS_URL,
    # æ·»åŠ æ›´å¤š RSS æºç¤ºä¾‹ï¼š
    # "ç§‘æŠ€æ–°é—»": "https://techcrunch.com/feed/",
    # "å­¦æœ¯è®ºæ–‡": "http://arxiv.org/rss/cs.AI",
    # "å®è§‚ç»æµ": "https://example.com/macro/feed/",
}


def fetch_rss(url, days=1):
    """ä» RSS æºè·å–æ•°æ®"""
    print(f"ğŸš€ æ­£åœ¨è·å– RSS æ•°æ®...")
    print(f"   æ•°æ®æº: {url}")
    print(f"   æ—¶é—´èŒƒå›´: æœ€è¿‘ {days} å¤©")

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/120.0.0.0 Safari/537.36",
        "Accept": "application/rss+xml, application/xml, text/xml",
    }

    req = urllib.request.Request(url, headers=headers, method="GET")

    # SSL Context
    ctx = ssl.create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE

    try:
        with urllib.request.urlopen(req, context=ctx, timeout=30) as response:
            xml_content = response.read().decode("utf-8")
    except urllib.error.HTTPError as e:
        print(f"âŒ HTTP é”™è¯¯: {e.code} - {e.reason}")
        return None
    except urllib.error.URLError as e:
        print(f"âŒ ç½‘ç»œé”™è¯¯: {e.reason}")
        return None
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        return None

    # è§£æ RSS XML
    try:
        root = ET.fromstring(xml_content)
    except ET.ParseError as e:
        print(f"âŒ XML è§£æé”™è¯¯: {e}")
        return None

    channel = root.find("channel")
    if channel is None:
        print("âŒ æœªæ‰¾åˆ° RSS channel")
        return None

    # è®¡ç®—æ—¶é—´è¿‡æ»¤é˜ˆå€¼
    cutoff_time = datetime.now(timezone.utc) - timedelta(days=days)

    items = []
    for item in channel.findall("item"):
        title = item.findtext("title", "").strip()
        link = item.findtext("link", "").strip()
        pub_date = item.findtext("pubDate", "").strip()
        description = item.findtext("description", "").strip()

        # å°è¯•è·å–å…¨æ–‡å†…å®¹ï¼ˆcontent:encodedï¼‰
        content = ""
        for child in item:
            if "encoded" in child.tag:
                content = (child.text or "").strip()
                break

        # è§£æå‘å¸ƒæ—¶é—´
        parsed_date = None
        if pub_date:
            try:
                # RSS æ ‡å‡†æ—¶é—´æ ¼å¼: "Mon, 10 Feb 2026 02:00:00 +0000"
                parsed_date = datetime.strptime(
                    pub_date, "%a, %d %b %Y %H:%M:%S %z"
                )
            except ValueError:
                try:
                    parsed_date = datetime.strptime(
                        pub_date, "%a, %d %b %Y %H:%M:%S %Z"
                    )
                    parsed_date = parsed_date.replace(tzinfo=timezone.utc)
                except ValueError:
                    pass

        # æ—¶é—´è¿‡æ»¤
        if parsed_date and parsed_date < cutoff_time:
            continue

        # æå–åˆ†ç±»æ ‡ç­¾
        categories = [
            cat.text for cat in item.findall("category") if cat.text
        ]

        items.append({
            "title": title,
            "link": link,
            "pub_date": pub_date,
            "categories": categories,
            "description": description,
            "content": content if content else description,
        })

    result = {
        "source": url,
        "fetched_at": datetime.now(timezone.utc).isoformat(),
        "time_range_days": days,
        "count": len(items),
        "data": items,
    }

    return result


def fetch_all_sources(days=1):
    """ä»æ‰€æœ‰é…ç½®çš„ RSS æºè·å–æ•°æ®"""
    all_items = []

    for name, url in RSS_SOURCES.items():
        print(f"\nğŸ“¡ [{name}]")
        result = fetch_rss(url, days)
        if result and result.get("data"):
            for item in result["data"]:
                item["source_name"] = name
            all_items.extend(result["data"])
            print(f"   âœ… è·å– {len(result['data'])} æ¡")
        else:
            print(f"   âš ï¸ æœªè·å–åˆ°æ•°æ®")

    return {
        "sources": list(RSS_SOURCES.keys()),
        "fetched_at": datetime.now(timezone.utc).isoformat(),
        "time_range_days": days,
        "count": len(all_items),
        "data": all_items,
    }


def main():
    parser = argparse.ArgumentParser(
        description="YMOS RSS æ•°æ®è·å–å·¥å…· - ä» RSS æºè·å–å¸‚åœºä¿¡æ¯"
    )
    parser.add_argument(
        "days",
        type=float,
        nargs="?",
        default=1,
        help="è·å–æœ€è¿‘ N å¤©çš„æ•°æ®ï¼ˆé»˜è®¤: 1ï¼‰",
    )
    parser.add_argument(
        "--url",
        default=None,
        help="æŒ‡å®šå•ä¸ª RSS æº URLï¼ˆä¸æŒ‡å®šåˆ™ä½¿ç”¨å…¨éƒ¨é…ç½®æºï¼‰",
    )
    parser.add_argument(
        "--output",
        default="financial_data.json",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: financial_data.jsonï¼‰",
    )

    args = parser.parse_args()

    print("=" * 50)
    print("YMOS RSS æ•°æ®è·å–å·¥å…·")
    print("=" * 50)

    if args.url:
        result = fetch_rss(args.url, args.days)
    else:
        result = fetch_all_sources(args.days)

    if result and result.get("count", 0) > 0:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜: {args.output}")
        print(f"âœ… å…±è·å– {result['count']} æ¡æ•°æ®")

        # åˆ†ç±»ç»Ÿè®¡
        if result.get("data"):
            cats = {}
            for item in result["data"]:
                for cat in item.get("categories", ["æœªåˆ†ç±»"]):
                    cats[cat] = cats.get(cat, 0) + 1
            if cats:
                print("\nğŸ“ åˆ†ç±»ç»Ÿè®¡:")
                for cat, num in sorted(cats.items()):
                    print(f"   {cat}: {num} æ¡")
    else:
        print("\nâš ï¸ æœªè·å–åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œ RSS æºåœ°å€")
        sys.exit(1)


if __name__ == "__main__":
    main()
